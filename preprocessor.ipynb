{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamSankarTheDeveloper/Question-generation-algorithm-based-on-blooms-taxonomy/blob/preprocessing/preprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVdIW9s3N5qR",
        "outputId": "3456309d-612f-4099-ec25-36153e4402e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neuralcoref'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 772 (delta 10), reused 16 (delta 7), pack-reused 748\u001b[K\n",
            "Receiving objects: 100% (772/772), 67.85 MiB | 26.54 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy<3.0.0,>=2.1.0\n",
            "  Downloading spacy-2.3.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython>=0.25 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.29.33)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.9)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.25.1)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.22.4)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.64.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (22.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n",
            "Installing collected packages: plac, srsly, catalogue, thinc, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.7\n",
            "    Uninstalling thinc-8.1.7:\n",
            "      Successfully uninstalled thinc-8.1.7\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.9 which is incompatible.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catalogue-1.0.2 plac-1.1.3 spacy-2.3.9 srsly-1.0.6 thinc-7.4.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/neuralcoref\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (1.22.4)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.79-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.25.1)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from neuralcoref==4.0) (2.3.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.64.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.7.9)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.10.1)\n",
            "Collecting botocore<1.30.0,>=1.29.79\n",
            "  Downloading botocore-1.29.79-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.79->boto3->neuralcoref==4.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.79->boto3->neuralcoref==4.0) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for neuralcoref\n",
            "Successfully installed boto3-1.26.79 botocore-1.29.79 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.26.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.0\n",
            "  Downloading spacy-2.3.0-cp38-cp38-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (1.22.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (1.1.3)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (2.25.1)\n",
            "Collecting thinc==7.4.1\n",
            "  Downloading thinc-7.4.1-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.3.0) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.0) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.0) (2022.12.7)\n",
            "Installing collected packages: blis, thinc, spacy\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.6\n",
            "    Uninstalling thinc-7.4.6:\n",
            "      Successfully uninstalled thinc-7.4.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.3.9\n",
            "    Uninstalling spacy-2.3.9:\n",
            "      Successfully uninstalled spacy-2.3.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.4.1 spacy-2.3.0 thinc-7.4.1\n",
            "2.3.0\n",
            "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
            "don't seem to have the spaCy package itself installed (maybe because you've\n",
            "built from source?), so installing the model dependencies would cause spaCy to\n",
            "be downloaded, which probably isn't what you want. If the model package has\n",
            "other dependencies, you'll have to install them manually.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40155833/40155833 [00:01<00:00, 26073424.62B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f15d8163550>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#note:restart the kernel if you had already imported spacy new version\n",
        "#date:21/02/2023\n",
        "############################\n",
        "#https://youtu.be/Sp6jZFHI02Y\n",
        "!git clone https://github.com/huggingface/neuralcoref.git \n",
        "import os\n",
        "os.chdir(\"/content/neuralcoref\")\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .  #'.' is reqd\n",
        "#######################\n",
        "!pip install spacy==2.3.0 \n",
        "import spacy #after restarting the kernel\n",
        "print(spacy. __version__)\n",
        "######################\n",
        "import spacy.cli \n",
        "spacy.cli.download(\"en\")\n",
        "#so that \"en\" works\n",
        "#OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
        "#https://stackoverflow.com/questions/62822737/oserror-e050-cant-find-model-de-it-doesnt-seem-to-be-a-shortcut-link-a\n",
        "######################\n",
        "# Load your usual SpaCy model (one of SpaCy English models)\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "#Add neural coref to SpaCy's pipe\n",
        "\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
        "#doc = nlp(text)\n",
        "\n",
        "#edited_text = doc._.coref_resolved\n",
        "#nouns = doc._.coref_clusters\n",
        "#return edited_text, noun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRmG8HO-4Xoj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import spacy\n",
        "nlp1 = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "6jFWCQPw4ZtP"
      },
      "outputs": [],
      "source": [
        "class preprocess:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def convert_para_to_listofsents(self,text):\n",
        "    doc=nlp1(text)\n",
        "    token_sentences=[]\n",
        "    string_sentences=[]\n",
        "    for sent in doc.sents:\n",
        "      token_sentences.append(sent)\n",
        "      string_sentences.append(sent.text)\n",
        "    return  string_sentences #,token_sentences,\n",
        "\n",
        "  def convert_to_noun_phrases(self,sentence):\n",
        "    doc =nlp1(sentence)\n",
        "    a=[]\n",
        "    for np in doc.noun_chunks:\n",
        "      a.append(np.text)\n",
        "    return a\n",
        "\n",
        "  def convert_to_tokens(self,sentence):\n",
        "    #nltk\n",
        "    token = word_tokenize(sentence)\n",
        "    for i in token:\n",
        "      token[i]=token[i].text\n",
        "    return token\n",
        "\n",
        "  def convert_to_stopwords(self,sentence):\n",
        "    sp=spacy.load(\"en_core_web_sm\")\n",
        "    all_stopwords = sp.Defaults.stop_words\n",
        "    text_tokens = word_tokenize(sentence)\n",
        "    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
        "    for i in tokens_without_sw:\n",
        "      tokens_without_sw[i]=tokens_without_sw[i].text\n",
        "    return tokens_without_sw\n",
        "\n",
        "  def convert_to_basewords(self,sentence):\n",
        "    a=[]\n",
        "    doc=nlp1(sentence)\n",
        "    for token in doc:\n",
        "      a.append(token.lemma_)\n",
        "    for i in range(len(a)):\n",
        "      a[i]=pst.stem(a[i])\n",
        "    return a\n",
        "    \n",
        "  def convert_listitems_to_lowercase(self,list1):\n",
        "    for i in range(len(list1)):\n",
        "      list1[i]=list1[i].cnvrt_to_lwr()\n",
        "    return list1\n",
        "\n",
        "  def replace_all_corefs(self,text): \n",
        "    #change it into different versions\n",
        "    #https://stackoverflow.com/questions/6570635/installing-multiple-versions-of-a-package-with-pip\n",
        "    doc = nlp(text)\n",
        "    edited_text = doc._.coref_resolved\n",
        "    nouns = doc._.coref_clusters\n",
        "    for i in range(len(nouns)):\n",
        "      nouns[i]=str(nouns[i]).split(\":\")[0]\n",
        "    return edited_text #, nouns\n",
        "\n",
        "  def convert_to_ngrams(self,sentence, n):\n",
        "    manygrams = list(ngrams(sentence.split(), n))\n",
        "    for i in manygrams:\n",
        "      manygrams[i]=manygrams[i].text\n",
        "    return manygrams\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX-ty_JmXdTe"
      },
      "outputs": [],
      "source": [
        "##############################################_diff_##############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8I902pEJKdo5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "dfcd9343-005c-49a3-a6f4-bb84cfe19b0f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-57dcb69456ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mcls_diff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seperating_and_synonyms_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_paragraph_to_0basewords_of_sentences_listoflist0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-57dcb69456ed>\u001b[0m in \u001b[0;36mcls_diff\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mcls_diff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seperating_and_synonyms_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_paragraph_to_0basewords_of_sentences_listoflist0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_seperating_and_synonyms_words() missing 1 required positional argument: 'self'"
          ]
        }
      ],
      "source": [
        "\n",
        "class cls_diff:\n",
        "  \n",
        " \n",
        "\n",
        "  def __init__(self, paragraph):\n",
        "    self.paragraph = paragraph\n",
        "    cls_diff.prepare_seperating_and_synonyms_words()\n",
        "    cls_diff.prepare_paragraph_to_0basewords_of_sentences_listoflist0()\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  def prepare_seperating_and_synonyms_words(self):\n",
        "    #prepare difference_synonyms_basewords_list and seperator_words for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "    obj_preprocess = preprocess(cls_diff.difference_synonyms_string)\n",
        "    cls_diff.difference_synonyms_basewords_list = obj_preprocess.convert_to_basewords(cls_diff.difference_synonyms_string)\n",
        "    cls_diff.seperator_words = obj_preprocess.convert_to_basewords(\"and versus vs ,\")\n",
        "\n",
        "  def prepare_paragraph_to_0basewords_of_sentences_listoflist0(self):\n",
        "    #text preperation into basewords for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "    text = self.paragraph\n",
        "    preprocess_obj = preprocess(text)\n",
        "    coref_text = preprocess_obj.replace_all_corefs(text)\n",
        "    sentences_list=preprocess_obj.convert_para_to_listofsents(text)\n",
        "    basewords_of_sentences_listoflist=[]\n",
        "    for each_sentence in sentences_list:\n",
        "      basewords_of_sentences_listoflist.append(preprocess_obj.convert_to_basewords(each_sentence))\n",
        "    cls_diff.basewords_of_sentences_listoflist = basewords_of_sentences_listoflist\n",
        "\n",
        "  def extract_sentences_that_contains_synonyms(self):\n",
        "    #returns the index if the synonym of \"difference\" is present in any of sentences. \n",
        "    # variables: #synonyms of the word \"difference\" = difference_synonyms_basewords_list , sentences = basewords_of_sentences_listoflist\n",
        "    basewords_of_sentences_listoflist = cls_diff.basewords_of_sentences_listoflist\n",
        "    for index in range(len(basewords_of_sentences_listoflist)):\n",
        "      for each_synonym in difference_synonyms_basewords_list:\n",
        "        if each_synonym in basewords_of_sentences_listoflist[index]:\n",
        "          #return index\n",
        "          cls_diff.split_sentences_with_between_etc(index)  #each iteration main function\n",
        "          \n",
        "  def split_sentences_with_between_etc(self, index):\n",
        "    if \"between\" in sentences_list[index].lower():\n",
        "      betw_splitted_sentence=str(sentences_list[index].split(\"between\")[1])\n",
        "      betw_splitted_sentence_words_listofstrngs=betw_splitted_sentence.split()\n",
        "      cls_diff.split_sententences_with_and_etc(betw_splitted_sentence)\n",
        "        #regex\n",
        "\n",
        "  def split_sententences_with_and_etc(self, betw_splitted_sentence):\n",
        "    #test_list = ['and', 'versus']\n",
        "    test_list = cls_diff.seperator_words\n",
        "    #betw_splitted_sentence = 'the main difference between power of z angle, x angle and y  angle is very interesting to teach'\n",
        "    # code to split the line with 'diff' with keywords and and comma\n",
        "    if re.search(r'\\b({})\\b'.format(\"|\".join(test_list)),betw_splitted_sentence):\n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]and[\\s]\", \"~\",betw_splitted_sentence )\n",
        "      splitted_parts_lstofstrngs = re.sub(\",[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]versus[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]vs[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "      splitted_parts_lstofstrngs = str(splitted_parts_lstofstrngs.split(\"between\")[1])\n",
        "      splitted_parts_lstofstrngs = re.split(\"~\",splitted_parts_lstofstrngs)\n",
        "    cls_diff.extract_dependency_from_splitted_parts(splitted_parts_lstofstrngs)\n",
        "\n",
        "  def extract_dependency_from_splitted_parts(self, splitted_parts_lstofstrngs):\n",
        "      #code to extract dependencies from parts splitted by seperating words.\n",
        "      words_and_deps_ofsplit_asdict={}\n",
        "      dep_pattern_ofsplits_asstrng=\"\"\n",
        "      words_and_deps_ofsplits_aslistofdict=[]\n",
        "      for each_splitted_part in splitted_parts_lstofstrngs:\n",
        "        for token in word_tokenize(each_splitted_part):\n",
        "          words_and_deps_ofsplit_asdict[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]  #0=>word, 1=>dep\n",
        "          dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+nltk.pos_tag([token])[0][1]\n",
        "        words_and_deps_ofsplits_aslistofdict.append(words_and_deps_ofsplit_asdict)\n",
        "        dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+\"~\"\n",
        "        words_and_deps_ofsplit_asdict={}\n",
        "      print(words_and_deps_ofsplits_aslistofdict)\n",
        "      dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng[:-1]\n",
        "      print(dep_pattern_ofallsplits_asstrng)\n",
        "      dep_pattern_ofallsplits_aslstofstrngs=dep_pattern_ofallsplits_asstrng.split(\"~\")\n",
        "      cls_diff.find_smallest_split_pattern(dep_pattern_ofallsplits_aslstofstrngs)\n",
        "\n",
        "\n",
        "  \n",
        "  def find_smallest_split_pattern(dep_pattern_ofallsplits_aslstofstrngs):\n",
        "    #choose smallest part of seperated line parts\n",
        "    smallest_diff_line=dep_pattern_ofallsplits_aslstofstrngs[0]\n",
        "    for i in dep_pattern_ofallsplits_aslstofstrngs[1:]:\n",
        "      if len(i)<len(smallest_diff_line):\n",
        "        smallest_diff_line=i\n",
        "    cls_diff.create_all_ordered_compinations_of_word(smallest_diff_line)\n",
        "\n",
        "  def create_all_ordered_compinations_of_word (self, pattern1): \n",
        "    #code to get each ordered compination of a sentence to check for a pattern from in descending order of length\n",
        "    for incr in range(len(pattern1)): #pattern1 is named so,  because renaming \"pattern\" might change other keywords with \"pattern\" in it\n",
        "      print(\"===\")\n",
        "      decr=len(pattern1)-incr-1\n",
        "      decr_cp=decr\n",
        "      #print(line[0:k])\n",
        "      sub_incr=0\n",
        "      while decr_cp !=len(pattern1):\n",
        "        #function check_pattern_is_present_in_all():\n",
        "        print(\"go ahead\")\n",
        "        #use regex, if present, output that pattern.\n",
        "\n",
        "        #print(pattern1[sub_incr:decr_cp+1]) #activate this line to print all ordered combinations of word in the terminal\n",
        "        decr_cp=decr_cp+1\n",
        "        sub_incr=sub_incr+1\n",
        "\n",
        "\n",
        "#check pattern present is present in all if found  chunkout that pattern using from the sentence and feed to the question\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#diff_list=[\"contrast\",\"difference\",\"distinction\",\"contrary\",\"different\",\"differing\",\"unlike\",\"differ\",\"unlike\",\"versus\",\"vs\",\"v/s\"]\n",
        "#diff_maybe_list=[\"the opposite\",\"the contrary\",\"the reverse\",\"compare\",\"on the other hand\",\"while\",\"whereas\",\"but\"]\n",
        "seperator_words=[\"and\",\",\",\"versus\",\"vs\"]\n",
        "#similarity_list=[\"resemblance similarity parallels analogy similar, analogous, common, comparable, identical, same, resemble, correspond, similarly, likewise, same, like, as\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x471eDJtNlkn",
        "outputId": "e4b0361b-85c3-44de-8d77-187045fb16fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n",
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n"
          ]
        }
      ],
      "source": [
        "text=\"many have splitted into two after world war two. There is still a sharp contrast between East and West Germany.One is ruled by western countries and one is ruled by soviet union\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#check if word is in sent\n",
        "for i in range(len(basewords_of_sentences_listoflist)):\n",
        "  for diffword in difference_synonyms_basewords_list:\n",
        "    if diffword in basewords_of_sentences_listoflist[i]:\n",
        "      #function1(basewords_of_sentences_listoflist[i],basewords_of_sentences_listoflist[i-1],basewords_of_sentences_listoflist[i+1])\n",
        "      if \"between\" in sentences_list[i].lower():\n",
        "        betw_split_sentence=str(sentences_list[i].split(\"between\")[1])\n",
        "        words_sentence=betw_split_sentence.split()\n",
        "#regex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9fNJBIPk0K7",
        "outputId": "5c546401-5447-4600-89ab-a0235a300ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the main difference between power of z angle~x angle~y  angle is very interesting to teach\n",
            " power of z angle~x angle~y  angle is very interesting to teach between\n",
            "[' power of z angle', 'x angle', 'y  angle is very interesting to teach']\n",
            "[' power of z angle', 'x angle', 'y  angle is very interesting to teach']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "test_list = ['and', 'versus']\n",
        "text1 = 'the main difference between power of z angle, x angle and y  angle is very interesting to teach'\n",
        "\n",
        "# code to split the line with 'diff' with keywords and and comma\n",
        "if re.search(r'\\b({})\\b'.format(\"|\".join(test_list)),text1):\n",
        "  txt=re.sub(\"[\\s]and[\\s]\", \"~\",text1 )\n",
        "  txt=re.sub(\",[\\s]\", \"~\", txt)\n",
        "  txt=re.sub(\"[\\s]versus[\\s]\", \"~\", txt)\n",
        "  txt=re.sub(\"[\\s]vs[\\s]\", \"~\", txt)\n",
        "  print(txt)\n",
        "  txt=str(txt.split(\"between\")[1])\n",
        "  print(txt,\"between\")\n",
        "  txt=re.split(\"~\",txt)\n",
        "  print(txt)\n",
        "print(txt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glMw2UwrlOLe",
        "outputId": "4929b62b-9064-4794-8079-46aeb64f88b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'power': 'NN', 'of': 'IN', 'z': 'NN', 'angle': 'NN'}, {'x': 'NN', 'angle': 'NN'}, {'y': 'NN', 'angle': 'NN', 'is': 'VBZ', 'very': 'RB', 'interesting': 'VBG', 'to': 'TO', 'teach': 'NN'}]\n",
            "NNINNNNN~NNNN~NNNNVBZRBVBGTONN\n",
            "['NNINNNNN', 'NNNN', 'NNNNVBZRBVBGTONN']\n",
            "NNNN\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#code to extract dependencies\n",
        "dict_a={}\n",
        "pattern_line=\"\"\n",
        "list_dict_a=[]\n",
        "for txtitem in txt:\n",
        "  for token in word_tokenize(txtitem):\n",
        "    dict_a[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]\n",
        "    pattern_line=pattern_line+nltk.pos_tag([token])[0][1]\n",
        "  list_dict_a.append(dict_a)\n",
        "  pattern_line=pattern_line+\"~\"\n",
        "  dict_a={}\n",
        "print(list_dict_a)\n",
        "pattern_line=pattern_line[:-1]   #to remove the last tilda\n",
        "print(pattern_line)  \n",
        "sep_diff_lines=pattern_line.split(\"~\")  #note:probably could seperate these in the previous for loop itself and simplify the code\n",
        "print(sep_diff_lines)\n",
        "\n",
        "#choose smallest part of seperated line parts\n",
        "smallest_diff_line=sep_diff_lines[0]\n",
        "for i in sep_diff_lines[1:]:\n",
        "  if len(i)<len(smallest_diff_line):\n",
        "    smallest_diff_line=i\n",
        "print(smallest_diff_line)\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WH4I_MUgZvWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O9nc8_gRNQKU"
      },
      "outputs": [],
      "source": [
        "#class_attributes\n",
        "  difference_synonyms_string = \"contrast difference distinction contrary different differing unlike differ unlike versus vs\"\n",
        "  obj_diff=preprocess(difference_synonyms_string)\n",
        "  difference_synonyms_basewords_list=obj_diff.convert_to_basewords(difference_synonyms_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DBj5h2gzkwG2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yVtYFJdjYVi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNy3O_5nT3JJ"
      },
      "outputs": [],
      "source": [
        "###################################################################_similarities_#######################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MV75YLerUMvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1ybmf-klFlF",
        "outputId": "1bda11b1-eb8e-4b5f-d1e4-e6252f157cf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'many have splitted into two after world war two. There is still a sharp contrast between East and West Germany.One is ruled by western countries and one is ruled by soviet union'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "QNNvWVTGmVAh",
        "outputId": "6807d266-8bcf-482c-fdc6-1988fff90bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2598ae6c76a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobjsmple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#coref_text=objsmple.replace_all_corefs(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentences_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_para_to_listofsents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmany_sent_basewords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_each\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'preprocess' object has no attribute 'convert_para_to_listofsents'"
          ]
        }
      ],
      "source": [
        "objsmple=preprocess(text)\n",
        "#coref_text=objsmple.replace_all_corefs(text)\n",
        "sentences_list=obj.convert_para_to_listofsents(text)\n",
        "many_sent_basewords=[]\n",
        "for sent_each in sentences_list:\n",
        "  many_sent_basewords.append(obj.convert_to_basewords(sent_each))\n",
        "many_sent_basewords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nS3WUDWqp5fo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0b12cf9e-1018-4d06-fa01-5e89daf933b3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6944c0015bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpattern_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_dict_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtxtitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdict_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'txt' is not defined"
          ]
        }
      ],
      "source": [
        "dict_a={}\n",
        "pattern_line=\"\"\n",
        "list_dict_a=[]\n",
        "for txtitem in txt:\n",
        "  for token in word_tokenize(txtitem):\n",
        "    dict_a[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]\n",
        "    pattern_line=pattern_line+nltk.pos_tag([token])[0][1]\n",
        "  list_dict_a.append(dict_a)\n",
        "  pattern_line=pattern_line+\"~\"\n",
        "  dict_a={}\n",
        "print(list_dict_a)\n",
        "pattern_line=pattern_line[:-1]\n",
        "print(pattern_line)\n",
        "sep_diff_lines=pattern_line.split(\"~\")\n",
        "print(sep_diff_lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_obj=cls_diff(\"there are many differences between apple, orange and mango\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "OgKsrXHUA3Hm",
        "outputId": "f279e89f-266c-4024-d193-196c204b645d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7fb8a516e375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"there are many differences between apple, orange and mango\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-23ee45f63b64>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, paragraph)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seperating_and_synonyms_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcls_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_paragraph_to_0basewords_of_sentences_listoflist0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_seperating_and_synonyms_words() missing 1 required positional argument: 'self'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class cls_diff:\n",
        "  \n",
        "  def __init__(self, paragraph):\n",
        "    self.paragraph = paragraph\n",
        "    self.difference_synonyms_string=\"difference\"\n",
        "    self.sentences_list=[]\n",
        "    self.list_of_questions=[]\n",
        "    self.between_splitted_sentence=\"aa\"\n",
        "    self.prepare_seperating_and_synonyms_words()\n",
        "    self.prepare_paragraph_to_0basewords_of_sentences_listoflist0()\n",
        "    self.extract_sentences_that_contains_synonyms()\n",
        "    self.basewords_of_sentences_listoflist\n",
        "    \n",
        "  def prepare_seperating_and_synonyms_words(self):\n",
        "    #prepare difference_synonyms_basewords_list and seperator_words for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "    obj_preprocess = preprocess(self.difference_synonyms_string)\n",
        "    self.difference_synonyms_basewords_list = obj_preprocess.convert_to_basewords(self.difference_synonyms_string)\n",
        "    self.seperator_words = obj_preprocess.convert_to_basewords(\"and versus vs ,\")\n",
        "    \n",
        "\n",
        "  def prepare_paragraph_to_0basewords_of_sentences_listoflist0(self):\n",
        "    #text preperation into basewords for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "    text = self.paragraph\n",
        "    preprocess_obj = preprocess(text)\n",
        "    coref_text = preprocess_obj.replace_all_corefs(text)\n",
        "    self.sentences_list=self.sentences_list+preprocess_obj.convert_para_to_listofsents(text)\n",
        "    basewords_of_sentences_listoflist=[]\n",
        "    for each_sentence in self.sentences_list:\n",
        "      basewords_of_sentences_listoflist.append(preprocess_obj.convert_to_basewords(each_sentence))\n",
        "    self.basewords_of_sentences_listoflist = basewords_of_sentences_listoflist\n",
        "\n",
        "  def extract_sentences_that_contains_synonyms(self):\n",
        "    #returns the index if the synonym of \"difference\" is present in any of sentences. \n",
        "    # variables: #synonyms of the word \"difference\" = difference_synonyms_basewords_list , sentences = basewords_of_sentences_listoflist\n",
        "    basewords_of_sentences_listoflist = self.basewords_of_sentences_listoflist\n",
        "    for index in range(len(basewords_of_sentences_listoflist)):\n",
        "      for each_synonym in self.difference_synonyms_basewords_list:\n",
        "        if each_synonym in basewords_of_sentences_listoflist[index]:\n",
        "          #return index\n",
        "          self.split_sentences_with_between_etc(index)  #each iteration main function\n",
        "          #print(\"index=\",index)\n",
        "          \n",
        "  def split_sentences_with_between_etc(self, index):\n",
        "    if \"between\" in self.sentences_list[index].lower():\n",
        "      betw_splitted_sentence=str(self.sentences_list[index].split(\"between\")[1])\n",
        "      self.betw_splitted_sentence=betw_splitted_sentence\n",
        "      betw_splitted_sentence_words_listofstrngs=betw_splitted_sentence.split()\n",
        "      self.split_sententences_with_and_etc(betw_splitted_sentence)\n",
        "        #regex\n",
        "\n",
        "  def split_sententences_with_and_etc(self, betw_splitted_sentence):\n",
        "    #test_list = ['and', 'versus']\n",
        "    test_list = self.seperator_words\n",
        "    #betw_splitted_sentence = 'the main difference between power of z angle, x angle and y  angle is very interesting to teach'\n",
        "    # code to split the line with 'diff' with keywords and and comma\n",
        "    if re.search(r'\\b({})\\b'.format(\"|\".join(test_list)),betw_splitted_sentence):\n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]and[\\s]\", \"~\",betw_splitted_sentence ) #change splittd parts lst of strings to before and after since this is a string\n",
        "      splitted_parts_lstofstrngs = re.sub(\",[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]versus[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "      \n",
        "      splitted_parts_lstofstrngs = re.sub(\"[\\s]vs[\\s]\", \"~\", splitted_parts_lstofstrngs)  #make between optional\n",
        "\n",
        "      # splitted_parts_lstofstrngs = str(splitted_parts_lstofstrngs.split(\"between\")[1]) #already splitted\n",
        "      splitted_parts_lstofstrngs = re.split(\"~\",splitted_parts_lstofstrngs)\n",
        "      self.extract_dependency_from_splitted_parts(splitted_parts_lstofstrngs)\n",
        "\n",
        "  def extract_dependency_from_splitted_parts(self, splitted_parts_lstofstrngs):\n",
        "      #code to extract dependencies from parts splitted by seperating words.\n",
        "      words_and_deps_ofsplit_asdict={}\n",
        "      dep_pattern_ofallsplits_asstrng=\"\"\n",
        "      words_and_deps_ofsplits_aslistofdict=[]\n",
        "      for each_splitted_part in splitted_parts_lstofstrngs:\n",
        "        for token in word_tokenize(each_splitted_part):\n",
        "          words_and_deps_ofsplit_asdict[nltk.pos_tag([token])[0][0]] = nltk.pos_tag([token])[0][1] #0=>word, 1=>dep\n",
        "          dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+str(\"<\")+nltk.pos_tag([token])[0][1]+str(\">\")\n",
        "        words_and_deps_ofsplits_aslistofdict.append(words_and_deps_ofsplit_asdict)\n",
        "        dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+\"~\"\n",
        "        words_and_deps_ofsplit_asdict={}\n",
        "      dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng[:-1]\n",
        "      dep_pattern_ofallsplits_aslstofstrngs=dep_pattern_ofallsplits_asstrng.split(\"~\")\n",
        "      self.find_smallest_split_pattern(dep_pattern_ofallsplits_aslstofstrngs)\n",
        "\n",
        "\n",
        "  \n",
        "  def find_smallest_split_pattern(self, dep_pattern_ofallsplits_aslstofstrngs):\n",
        "    #choose smallest part of seperated line parts\n",
        "    smallest_diff_line=dep_pattern_ofallsplits_aslstofstrngs[0]\n",
        "    for i in dep_pattern_ofallsplits_aslstofstrngs[1:]:\n",
        "      if len(i)<len(smallest_diff_line):\n",
        "        smallest_diff_line=i\n",
        "    largest_common_pattern_of_diff=self.create_all_ordered_compinations_of_word(smallest_diff_line)\n",
        "    self.match_deppattern_in_sentence(largest_common_pattern_of_diff)\n",
        "\n",
        "\n",
        "\n",
        "  def create_all_ordered_compinations_of_word (self, pattern1): \n",
        "    #code to get each ordered compination of a sentence to check for a pattern from in descending order of length\n",
        "    for incr in range(len(pattern1)): #pattern1 is named so,  because renaming \"pattern\" might change other keywords with \"pattern\" in it\n",
        "      decr=len(pattern1)-incr-1\n",
        "      decr_cp=decr\n",
        "      #print(line[0:k])\n",
        "      sub_incr=0\n",
        "      while decr_cp !=len(pattern1):\n",
        "        return pattern1\n",
        "        #use regex, if present, output that pattern.\n",
        "        #print(pattern1[sub_incr:decr_cp+1]) #activate this line to print all ordered combinations of word in the terminal\n",
        "        decr_cp=decr_cp+1\n",
        "        sub_incr=sub_incr+1\n",
        "\n",
        "  def match_deppattern_in_sentence(self,pattern1):\n",
        "    token = word_tokenize(self.betw_splitted_sentence)\n",
        "    tags = nltk.pos_tag(token)\n",
        "    reg = \"NN:{\" + str(pattern1) + \"}\"\n",
        "    a = nltk.RegexpParser(reg)\n",
        "    result = a.parse(tags)\n",
        "    list_of_contexts_ineachsentence=[]\n",
        "    for subtree in result.subtrees():\n",
        "      a=\" \".join(w for w, t in subtree.leaves())\n",
        "      list_of_contexts_ineachsentence.append( a ) \n",
        "    self.create_question_and_save(list_of_contexts_ineachsentence[1:])\n",
        "\n",
        "  def create_question_and_save(self, list_of_contexts_ineachsentence_cpy):\n",
        "    for i in range(len(list_of_contexts_ineachsentence_cpy)-1):\n",
        "      list_of_contexts_ineachsentence_cpy[i] = list_of_contexts_ineachsentence_cpy[i] + str(\",\")\n",
        "\n",
        "    list_of_contexts_ineachsentence_cpy[-1] = str(\"and \") + list_of_contexts_ineachsentence_cpy[-1]\n",
        "    \n",
        "    self.list_of_questions.append(str(\"What are the differences between \" + \" \".join(list_of_contexts_ineachsentence_cpy)))\n",
        "\n",
        " # def diff_questions_list(self):\n",
        "  #  return self.list_of_questions\n",
        "\n",
        "\n",
        "\n",
        "#check pattern present is present in all if found  chunkout that pattern using from the sentence and feed to the question\n",
        "\n",
        "\n",
        "\n",
        "#diff_list=[\"contrast\",\"difference\",\"distinction\",\"contrary\",\"different\",\"differing\",\"unlike\",\"differ\",\"unlike\",\"versus\",\"vs\",\"v/s\"]\n",
        "#diff_maybe_list=[\"the opposite\",\"the contrary\",\"the reverse\",\"compare\",\"on the other hand\",\"while\",\"whereas\",\"but\"]\n",
        "#seperator_words=[\"and\",\",\",\"versus\",\"vs\"]\n",
        "#similarity_list=[\"resemblance similarity parallels analogy similar, analogous, common, comparable, identical, same, resemble, correspond, similarly, likewise, same, like, as\"]"
      ],
      "metadata": {
        "id": "nMaQUqp067ck"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_obj=cls_diff( 'The main difference between a good book and a bad book is its difference. The main difference between a well written story and a poorly written story is its story.')\n",
        "print(test_obj.list_of_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQpb3PuNChTW",
        "outputId": "b1784db6-a764-45dd-a693-806daaeda227"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What are the differences between a good book, and a bad book', 'What are the differences between a well written story, and a poorly written story']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"We saw the big yellow dog\"\n",
        "token = word_tokenize(text)\n",
        "print(\"tokens=\",token)\n",
        "tags = nltk.pos_tag(token)\n",
        "print(tags,\"=tags\")\n",
        "reg = \"NP:{<DT>?<JJ>*<NN>}\"\n",
        "a = nltk.RegexpParser(reg)\n",
        "result = a.parse(tags)\n",
        "for subtree in result.subtrees():\n",
        "                print(subtree)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGgbj18YClOT",
        "outputId": "94f8647a-6009-41a4-da16-3c72f60d8a4b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens= ['We', 'saw', 'the', 'big', 'yellow', 'dog']\n",
            "[('We', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('big', 'JJ'), ('yellow', 'NN'), ('dog', 'NN')] =tags\n",
            "(S We/PRP saw/VBD (NP the/DT big/JJ yellow/NN) (NP dog/NN))\n",
            "(NP the/DT big/JJ yellow/NN)\n",
            "(NP dog/NN)\n",
            "(S We/PRP saw/VBD (NP the/DT big/JJ yellow/NN) (NP dog/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze98iQgFyyo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def    ():\n",
        "  token = word_tokenize(text)\n",
        "  tags = nltk.pos_tag(token)\n",
        "  reg = \"NP:{<DT>?<JJ>*<NN>}\"\n",
        "  a = nltk.RegexpParser(reg)\n",
        "  result = a.parse(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "PeggzDWo5IQg",
        "outputId": "e6e0fa53-69e1-4b20-8c9b-b14d20f7edd2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-77ec66e6dcf5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def    ():\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"ny have splitted into two after world war two. There is still a sharp contrast between East and West Germany.One is ruled by western countries and one is ruled by soviet union\"\n",
        "nounphrases = []\n",
        "words = nltk.word_tokenize(text)\n",
        "tagged = nltk.pos_tag(words)\n",
        "grammar = \"NN:{\" + str(\"<NN>\") + \"}\"\n",
        "chunkParser = nltk.RegexpParser(grammar)\n",
        "tree = chunkParser.parse(tagged)\n",
        "for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "    myPhrase = ''\n",
        "    for item in subtree.leaves():\n",
        "        myPhrase += ' ' + item[0]\n",
        "    nounphrases.append(myPhrase.strip())\n",
        "    print(myPhrase)\n",
        "nounphrases = list(filter(lambda x: len(x.split()) > 1, nounphrases))\n",
        "nounphrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nCvGWbf5JCY",
        "outputId": "7c752fde-1691-4f4b-b356-38600d654636"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subtree in chunked.subtrees():\n",
        "                print(subtree)"
      ],
      "metadata": {
        "id": "B5e8H6RYE4o5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlH1Ip5AcOg1w7nUQ6bDFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}