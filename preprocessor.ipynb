{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamSankarTheDeveloper/Question-generation-algorithm-based-on-blooms-taxonomy/blob/preprocessing/preprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVdIW9s3N5qR"
      },
      "outputs": [],
      "source": [
        "#note:restart the kernel if you had already imported spacy new version\n",
        "#date:21/02/2023\n",
        "############################\n",
        "#https://youtu.be/Sp6jZFHI02Y\n",
        "!git clone https://github.com/huggingface/neuralcoref.git \n",
        "import os\n",
        "os.chdir(\"/content/neuralcoref\")\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .  #'.' is reqd\n",
        "#######################\n",
        "!pip install spacy==2.3.0 \n",
        "import spacy #after restarting the kernel\n",
        "print(spacy. __version__)\n",
        "######################\n",
        "import spacy.cli \n",
        "spacy.cli.download(\"en\")\n",
        "#so that \"en\" works\n",
        "#OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
        "#https://stackoverflow.com/questions/62822737/oserror-e050-cant-find-model-de-it-doesnt-seem-to-be-a-shortcut-link-a\n",
        "######################\n",
        "# Load your usual SpaCy model (one of SpaCy English models)\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "#Add neural coref to SpaCy's pipe\n",
        "\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
        "#doc = nlp(text)\n",
        "\n",
        "#edited_text = doc._.coref_resolved\n",
        "#nouns = doc._.coref_clusters\n",
        "#return edited_text, noun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRmG8HO-4Xoj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import spacy\n",
        "nlp1 = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6jFWCQPw4ZtP"
      },
      "outputs": [],
      "source": [
        "class preprocess:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "    print(\"convert_para_to_listofsents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\")\n",
        "\n",
        "  def convert_para_to_listofsents(self,text):\n",
        "    doc=nlp1(text)\n",
        "    token_sentences=[]\n",
        "    string_sentences=[]\n",
        "    for sent in doc.sents:\n",
        "      token_sentences.append(sent)\n",
        "      string_sentences.append(sent.text)\n",
        "    return  string_sentences #,token_sentences,\n",
        "\n",
        "  def convert_to_noun_phrases(self,sentence):\n",
        "    doc =nlp1(sentence)\n",
        "    a=[]\n",
        "    for np in doc.noun_chunks:\n",
        "      a.append(np.text)\n",
        "    return a\n",
        "\n",
        "  def convert_to_tokens(self,sentence):\n",
        "    #nltk\n",
        "    token = word_tokenize(sentence)\n",
        "    for i in token:\n",
        "      token[i]=token[i].text\n",
        "    return token\n",
        "\n",
        "  def convert_to_stopwords(self,sentence):\n",
        "    sp=spacy.load(\"en_core_web_sm\")\n",
        "    all_stopwords = sp.Defaults.stop_words\n",
        "    text_tokens = word_tokenize(sentence)\n",
        "    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
        "    for i in tokens_without_sw:\n",
        "      tokens_without_sw[i]=tokens_without_sw[i].text\n",
        "    return tokens_without_sw\n",
        "\n",
        "  def convert_to_basewords(self,sentence):\n",
        "    a=[]\n",
        "    doc=nlp1(sentence)\n",
        "    for token in doc:\n",
        "      a.append(token.lemma_)\n",
        "    for i in range(len(a)):\n",
        "      a[i]=pst.stem(a[i])\n",
        "    return a\n",
        "    \n",
        "  def convert_listitems_to_lowercase(self,list1):\n",
        "    for i in range(len(list1)):\n",
        "      list1[i]=list1[i].cnvrt_to_lwr()\n",
        "    return list1\n",
        "\n",
        "  def replace_all_corefs(self,text): \n",
        "    #change it into different versions\n",
        "    #https://stackoverflow.com/questions/6570635/installing-multiple-versions-of-a-package-with-pip\n",
        "    doc = nlp(text)\n",
        "    edited_text = doc._.coref_resolved\n",
        "    nouns = doc._.coref_clusters\n",
        "    for i in range(len(nouns)):\n",
        "      nouns[i]=str(nouns[i]).split(\":\")[0]\n",
        "    return edited_text #, nouns\n",
        "\n",
        "  def convert_to_ngrams(self,sentence, n):\n",
        "    manygrams = list(ngrams(sentence.split(), n))\n",
        "    for i in manygrams:\n",
        "      manygrams[i]=manygrams[i].text\n",
        "    return manygrams\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX-ty_JmXdTe"
      },
      "outputs": [],
      "source": [
        "##############################################_diff_##############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8I902pEJKdo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc441f4-df6e-4a04-e988-a71876f19ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class cls_diff:\n",
        " \n",
        "\n",
        "  def __init__(self, paragraph):\n",
        "    self.paragraph = paragraph\n",
        "    cls_diff.prepare_seperating_and_synonyms_words()\n",
        "    cls_diff.prepare_paragraph_to_0basewords_of_sentences_listoflist0()\n",
        "    cls.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def prepare_seperating_and_synonyms_words(self):\n",
        "  #prepare difference_synonyms_basewords_list and seperator_words for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "  obj_diff = preprocess(cls_diff.difference_synonyms_string)\n",
        "  cls_diff.difference_synonyms_basewords_list = obj_diff.convert_to_basewords(cls_diff.difference_synonyms_string)\n",
        "  cls_diff.seperator_words = obj_diff.convert_to_basewords(\"and versus vs ,\")\n",
        "\n",
        "def prepare_paragraph_to_0basewords_of_sentences_listoflist0(self):\n",
        "  #text preperation into basewords for matching (check_0difference_synonyms_basewords_list0_present_in_0basewords_of_sentences_listoflist0():)\n",
        "  text = self.paragraph\n",
        "  preprocess_obj = preprocess(text)\n",
        "  coref_text = preprocess_obj.replace_all_corefs(text)\n",
        "  sentences_list=preprocess_obj.convert_para_to_listofsents(text)\n",
        "  basewords_of_sentences_listoflist=[]\n",
        "  for each_sentence in sentences_list:\n",
        "    basewords_of_sentences_listoflist.append(preprocess_obj.convert_to_basewords(each_sentence))\n",
        "  cls_diff.basewords_of_sentences_listoflist = basewords_of_sentences_listoflist\n",
        "\n",
        "def extract_sentences_containing_synonyms(self):\n",
        "  #returns the index if the synonym of \"difference\" is present in any of sentences. \n",
        "  # variables: #synonyms of the word \"difference\" = difference_synonyms_basewords_list , sentences = basewords_of_sentences_listoflist\n",
        "  basewords_of_sentences_listoflist = cls_diff.basewords_of_sentences_listoflist\n",
        "  for index in range(len(basewords_of_sentences_listoflist)):\n",
        "    for each_synonym in difference_synonyms_basewords_list:\n",
        "      if each_synonym in basewords_of_sentences_listoflist[index]:\n",
        "        return index\n",
        "          \n",
        "def split_sentences_with_between_etc():\n",
        "  if \"between\" in sentences_list[i].lower():\n",
        "    betw_split_sentence=str(sentences_list[i].split(\"between\")[1])\n",
        "    words_sentence_listofstrngs=betw_split_sentence.split()\n",
        "      #regex\n",
        "\n",
        "def split_sententences_with_and_etc():\n",
        "  import re\n",
        "#test_list = ['and', 'versus']\n",
        "test_list = cls_diff.seperator_words\n",
        "betw_split_sentence = 'the main difference between power of z angle, x angle and y  angle is very interesting to teach'\n",
        "# code to split the line with 'diff' with keywords and and comma\n",
        "if re.search(r'\\b({})\\b'.format(\"|\".join(test_list)),betw_split_sentence):\n",
        "  splitted_parts_lstofstrngs = re.sub(\"[\\s]and[\\s]\", \"~\",betw_split_sentence )\n",
        "  splitted_parts_lstofstrngs = re.sub(\",[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "  splitted_parts_lstofstrngs = re.sub(\"[\\s]versus[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "  splitted_parts_lstofstrngs = re.sub(\"[\\s]vs[\\s]\", \"~\", splitted_parts_lstofstrngs)\n",
        "  splitted_parts_lstofstrngs = str(splitted_parts_lstofstrngs.split(\"between\")[1])\n",
        "  splitted_parts_lstofstrngs = re.split(\"~\",splitted_parts_lstofstrngs)\n",
        "print(splitted_parts_lstofstrngs)\n",
        "\n",
        "#code to extract dependencies from parts splitted by seperating words.\n",
        "words_and_deps_ofsplit_asdict={}\n",
        "dep_pattern_ofsplits_asstrng=\"\"\n",
        "words_and_deps_ofsplits_aslistofdict=[]\n",
        "for each_splitted_part in splitted_parts_lstofstrngs:\n",
        "  for token in word_tokenize(each_splitted_part):\n",
        "    words_and_deps_ofsplit_asdict[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]  #0=>word, 1=>dep\n",
        "    dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+nltk.pos_tag([token])[0][1]\n",
        "  words_and_deps_ofsplits_aslistofdict.append(words_and_deps_ofsplit_asdict)\n",
        "  dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng+\"~\"\n",
        "  words_and_deps_ofsplit_asdict={}\n",
        "print(words_and_deps_ofsplits_aslistofdict)\n",
        "dep_pattern_ofallsplits_asstrng=dep_pattern_ofallsplits_asstrng[:-1]\n",
        "print(dep_pattern_ofallsplits_asstrng)\n",
        "dep_pattern_ofallsplits_aslstofstrngs=dep_pattern_ofallsplits_asstrng.split(\"~\")\n",
        "print(dep_pattern_ofallsplits_aslstofstrngs)\n",
        "\n",
        "#choose smallest part of seperated line parts\n",
        "smallest_diff_line=dep_pattern_ofallsplits_aslstofstrngs[0]\n",
        "for i in dep_pattern_ofallsplits_aslstofstrngs[1:]:\n",
        "  if len(i)<len(smallest_diff_line):\n",
        "    smallest_diff_line=i\n",
        "print(smallest_diff_line)\n",
        "\n",
        "def create_all_ordered_compinations_of_word (self,word):\n",
        "  #code to get each ordered compination of a sentence to check for a pattern from in descending order of length\n",
        "  for incr in range(len(word)):\n",
        "    print(\"===\")\n",
        "    decr=len(word)-incr-1\n",
        "    decr_cp=decr\n",
        "    #print(line[0:k])\n",
        "    sub_incr=0\n",
        "    while decr_cp !=len(word):\n",
        "      function check_word_is_present_in_all()\n",
        "      #print(word[sub_incr:decr_cp+1]) #activate this line to print all ordered combinations of word in the terminal\n",
        "      decr_cp=decr_cp+1\n",
        "      sub_incr=sub_incr+1\n",
        "\n",
        "function to compare,output=> a pettern\n",
        "function to select that pattern\n",
        "\n",
        "\n",
        "#diff_list=[\"contrast\",\"difference\",\"distinction\",\"contrary\",\"different\",\"differing\",\"unlike\",\"differ\",\"unlike\",\"versus\",\"vs\",\"v/s\"]\n",
        "#diff_maybe_list=[\"the opposite\",\"the contrary\",\"the reverse\",\"compare\",\"on the other hand\",\"while\",\"whereas\",\"but\"]\n",
        "seperator_words=[\"and\",\",\",\"versus\",\"vs\"]\n",
        "#similarity_list=[\"resemblance similarity parallels analogy similar, analogous, common, comparable, identical, same, resemble, correspond, similarly, likewise, same, like, as\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x471eDJtNlkn",
        "outputId": "e4b0361b-85c3-44de-8d77-187045fb16fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n",
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n"
          ]
        }
      ],
      "source": [
        "text=\"many have splitted into two after world war two. There is still a sharp contrast between East and West Germany.One is ruled by western countries and one is ruled by soviet union\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#check if word is in sent\n",
        "for i in range(len(basewords_of_sentences_listoflist)):\n",
        "  for diffword in difference_synonyms_basewords_list:\n",
        "    if diffword in basewords_of_sentences_listoflist[i]:\n",
        "      #function1(basewords_of_sentences_listoflist[i],basewords_of_sentences_listoflist[i-1],basewords_of_sentences_listoflist[i+1])\n",
        "      if \"between\" in sentences_list[i].lower():\n",
        "        betw_split_sentence=str(sentences_list[i].split(\"between\")[1])\n",
        "        words_sentence=betw_split_sentence.split()\n",
        "#regex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9fNJBIPk0K7",
        "outputId": "aa81a97b-f103-4e98-fda0-3cbf476def31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' power of z angle', 'x angle', 'y  angle is very interesting to teach']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "test_list = ['and', 'versus']\n",
        "text1 = 'the main difference between power of z angle, x angle and y  angle is very interesting to teach'\n",
        "\n",
        "# code to split the line with 'diff' with keywords and and comma\n",
        "if re.search(r'\\b({})\\b'.format(\"|\".join(test_list)),text1):\n",
        "  txt=re.sub(\"[\\s]and[\\s]\", \"~\",text1 )\n",
        "  txt=re.sub(\",[\\s]\", \"~\", txt)\n",
        "  txt=re.sub(\"[\\s]versus[\\s]\", \"~\", txt)\n",
        "  txt=re.sub(\"[\\s]vs[\\s]\", \"~\", txt)\n",
        "  txt=str(txt.split(\"between\")[1])\n",
        "  txt=re.split(\"~\",txt)\n",
        "print(txt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glMw2UwrlOLe",
        "outputId": "4929b62b-9064-4794-8079-46aeb64f88b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'power': 'NN', 'of': 'IN', 'z': 'NN', 'angle': 'NN'}, {'x': 'NN', 'angle': 'NN'}, {'y': 'NN', 'angle': 'NN', 'is': 'VBZ', 'very': 'RB', 'interesting': 'VBG', 'to': 'TO', 'teach': 'NN'}]\n",
            "NNINNNNN~NNNN~NNNNVBZRBVBGTONN\n",
            "['NNINNNNN', 'NNNN', 'NNNNVBZRBVBGTONN']\n",
            "NNNN\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#code to extract dependencies\n",
        "dict_a={}\n",
        "pattern_line=\"\"\n",
        "list_dict_a=[]\n",
        "for txtitem in txt:\n",
        "  for token in word_tokenize(txtitem):\n",
        "    dict_a[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]\n",
        "    pattern_line=pattern_line+nltk.pos_tag([token])[0][1]\n",
        "  list_dict_a.append(dict_a)\n",
        "  pattern_line=pattern_line+\"~\"\n",
        "  dict_a={}\n",
        "print(list_dict_a)\n",
        "pattern_line=pattern_line[:-1]   #to remove the last tilda\n",
        "print(pattern_line)  \n",
        "sep_diff_lines=pattern_line.split(\"~\")  #note:probably could seperate these in the previous for loop itself and simplify the code\n",
        "print(sep_diff_lines)\n",
        "\n",
        "#choose smallest part of seperated line parts\n",
        "smallest_diff_line=sep_diff_lines[0]\n",
        "for i in sep_diff_lines[1:]:\n",
        "  if len(i)<len(smallest_diff_line):\n",
        "    smallest_diff_line=i\n",
        "print(smallest_diff_line)\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WH4I_MUgZvWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O9nc8_gRNQKU"
      },
      "outputs": [],
      "source": [
        "#class_attributes\n",
        "  difference_synonyms_string = \"contrast difference distinction contrary different differing unlike differ unlike versus vs\"\n",
        "  obj_diff=preprocess(difference_synonyms_string)\n",
        "  difference_synonyms_basewords_list=obj_diff.convert_to_basewords(difference_synonyms_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DBj5h2gzkwG2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yVtYFJdjYVi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNy3O_5nT3JJ"
      },
      "outputs": [],
      "source": [
        "###################################################################_similarities_#######################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MV75YLerUMvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1ybmf-klFlF",
        "outputId": "1bda11b1-eb8e-4b5f-d1e4-e6252f157cf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'many have splitted into two after world war two. There is still a sharp contrast between East and West Germany.One is ruled by western countries and one is ruled by soviet union'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "QNNvWVTGmVAh",
        "outputId": "6807d266-8bcf-482c-fdc6-1988fff90bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convert_para_to_sents(text)=>sentslist, convert_to_noun_phrases(sent)=>phraselist,stopwords(sent)=>list/stpwrd removed wrdlist,basewords,cnvrt_to_lwr,replace_all_corefs,convert_to_ngrams\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2598ae6c76a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobjsmple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#coref_text=objsmple.replace_all_corefs(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentences_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_para_to_listofsents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmany_sent_basewords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_each\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'preprocess' object has no attribute 'convert_para_to_listofsents'"
          ]
        }
      ],
      "source": [
        "objsmple=preprocess(text)\n",
        "#coref_text=objsmple.replace_all_corefs(text)\n",
        "sentences_list=obj.convert_para_to_listofsents(text)\n",
        "many_sent_basewords=[]\n",
        "for sent_each in sentences_list:\n",
        "  many_sent_basewords.append(obj.convert_to_basewords(sent_each))\n",
        "many_sent_basewords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nS3WUDWqp5fo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0b12cf9e-1018-4d06-fa01-5e89daf933b3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6944c0015bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpattern_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_dict_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtxtitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdict_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'txt' is not defined"
          ]
        }
      ],
      "source": [
        "dict_a={}\n",
        "pattern_line=\"\"\n",
        "list_dict_a=[]\n",
        "for txtitem in txt:\n",
        "  for token in word_tokenize(txtitem):\n",
        "    dict_a[nltk.pos_tag([token])[0][0]]=nltk.pos_tag([token])[0][1]\n",
        "    pattern_line=pattern_line+nltk.pos_tag([token])[0][1]\n",
        "  list_dict_a.append(dict_a)\n",
        "  pattern_line=pattern_line+\"~\"\n",
        "  dict_a={}\n",
        "print(list_dict_a)\n",
        "pattern_line=pattern_line[:-1]\n",
        "print(pattern_line)\n",
        "sep_diff_lines=pattern_line.split(\"~\")\n",
        "print(sep_diff_lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OgKsrXHUA3Hm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEcASCkVAOVSXovLaXGxeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}